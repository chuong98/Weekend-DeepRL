{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Q-Learning (DQN)\n",
    "\n",
    "### 1. Motivation\n",
    "+ In Q-Learning, we discretize the state spaces, and use a table to record and update the Q-function. \n",
    "+ However, most of interesting problems are too large to learn all action values in all states separately. Instead, we can learn a parameterized value function $Q(s,a|\\theta_t)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Deep Q-Learning\n",
    "Deep Q-learning uses a Deep Neural Network to approximate Q-function. \n",
    "+ For a given state $s$, it outputs a vector of action values $Q(s, \\cdot|\\theta_t)$. For an $n-$dimentional state space and an action space containing $m$ actions, the neural network is a function from $R^n$ to $R^m$. \n",
    "+ Similar to the original Q-Learning, DQN updates the parameters after taking action $a_t$ in the state $s_t$, and observing the immediate reward $r_{t}$ and resulting state $s_{t+1}$. Concretely:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\frac{1}{2}\\nabla _{\\theta}(Q_{target}(s_{t+1},a|\\theta) - Q(s_t,a_t|\\theta))^2$$\n",
    "where\n",
    "$$ Q_{target}=r_t + \\gamma \\max_a Q(s_{t+1},a, \\theta) $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 3. What makes DQN work?\n",
    "Two important ingredients of DQN algorithm as proposed by Mnih et. al. (2015) are the use of:\n",
    "+ **Target Network**, with parameters $\\theta^-$, is the same as the online network except that its parameters are copied every $\\tau$ steps from the online network, so that $\\theta^- \\leftarrow \\theta$ if $t \\% \\tau==0$, and keep fixed on all other steps. The target used by DQN is then:\n",
    "    $$Q_{target}=r_t + \\gamma \\max_a Q(s_{t+1},a, \\theta^-) $$\n",
    "    In other words, we **freeze the target network** for $\\tau$ steps.\n",
    "+ **Experience Replay**: observed transitions $(s_t,a_t,r_t,s_{t+1})$ are stored for some time and **sampled uniformly** from this memory bank to update the network. This is because, DNN needs to be trained with mini-batch and SDG-like optimizer. If we use only a single sample, e.g the most current one, the network will be easily overfitted, and it cann't generalizes to all the states it saw in the past.  \n",
    "\n",
    "### 4. DQN Pseudo Code\n",
    "<p align=\"middle\">\n",
    "  <img src=\"Fig/DQN_pseudo.png\" width=\"80%\" /> \n",
    "</p>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}